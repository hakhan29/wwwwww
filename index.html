<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>웹캠 표정 인식</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            margin: 0;
            height: 100vh;
            font-family: Arial, sans-serif;
        }
        video {
            border: 2px solid #000;
            border-radius: 10px;
        }
        #expression {
            margin-top: 20px;
            font-size: 1.5rem;
            color: #333;
        }
    </style>
</head>
<body>
    <h1>웹캠 표정 인식</h1>
    <video id="video" autoplay muted></video>
    <div id="expression">표정을 분석 중...</div>

    <script>


        // video 요소와 텍스트 요소 가져오기
const video = document.getElementById('video');
const expressionDisplay = document.getElementById('expression');

// Face-api.js 모델 로드
Promise.all([
    faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
    faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
    faceapi.nets.faceExpressionNet.loadFromUri('/models')
]).then(startVideo);

// 웹캠 시작 함수
function startVideo() {
    navigator.mediaDevices.getUserMedia({ video: {} })
        .then(stream => {
            video.srcObject = stream;
        })
        .catch(err => console.error('웹캠 시작 중 오류 발생:', err));
}

// 웹캠에서 매 프레임마다 표정 분석
video.addEventListener('play', () => {
    const canvas = faceapi.createCanvasFromMedia(video);
    document.body.append(canvas);
    const displaySize = { width: video.width, height: video.height };
    faceapi.matchDimensions(canvas, displaySize);

    setInterval(async () => {
        const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceExpressions();

        canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
        const resizedDetections = faceapi.resizeResults(detections, displaySize);
        faceapi.draw.drawDetections(canvas, resizedDetections);

        if (detections[0]?.expressions) {
            const expressions = detections[0].expressions;
            const maxExpression = Object.keys(expressions).reduce((a, b) => expressions[a] > expressions[b] ? a : b);
            expressionDisplay.innerText = `표정: ${maxExpression}`;
        } else {
            expressionDisplay.innerText = '얼굴을 감지할 수 없습니다.';
        }
    }, 100);
});

    </script>
</body>
</html>
